{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1\n",
    "\n",
    "Лабораторную работу необходимо выполнять в ***Google Colab***. Результат работы (в виде ссылки на notebook) выслать письмом на litvinov.vg@ssau.ru. В теме письма указывать ФИО полностью.\n",
    "\n",
    "Для выполнения задания необходимо подобрать корпус текстов (художественных) на английском языке. Объем корпуса должен составлять не менее $3 \\cdot 10^7$ символов. Далее все действия будут выполняться над выбранными данными. Для отладки алгоритмов лучше использовать не весь корпус, а лишь его часть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код загрузки корпуса\n",
    "data = []\n",
    "with open('./book-war-and-peace.txt', mode='r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №1\n",
    "Заменить все числа, которые представлены цифрами, их текстовым представлением (т.е. прописью). (1 = one, 23 = twenty three, 1042 = one thousand forty two, и т.п.). Методы библиотек не использовать, алгоритм необходимо написать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "digit_2_text = {1:'one',\n",
    "                2:'two',\n",
    "                3:'three',\n",
    "                4:'four',\n",
    "                5:'five',\n",
    "                6:'six',\n",
    "                7:'seven',\n",
    "                8:'eight',\n",
    "                9:'nine'}\n",
    "\n",
    "teen_2_text = { 10:'ten',\n",
    "                11:'eleven',\n",
    "                12:'twelve',\n",
    "                13:'thirteen',\n",
    "                14:'fourteen',\n",
    "                15:'fifteen',\n",
    "                16:'sixteen',\n",
    "                17:'seventeen',\n",
    "                18:'eighteen',\n",
    "                19:'nineteen'}\n",
    "\n",
    "tens_2_text =  {20:'twenty',\n",
    "                30:'thirty',\n",
    "                40:'fourty',\n",
    "                50:'fifty',\n",
    "                60:'sixty',\n",
    "                70:'seventy',\n",
    "                80:'eighty',\n",
    "                90:'ninety'}\n",
    "\n",
    "power_2_text = {100:'hundred',\n",
    "                1000:'thousand',\n",
    "                10**6:'million',\n",
    "                10**9:'billion',\n",
    "                10**12:'trillion',\n",
    "                10**15:'quadrillion',\n",
    "                10**18:'quintillion'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2text(number):\n",
    "    text = ''\n",
    "    order_of_magnitude = len(str(number))\n",
    "    if order_of_magnitude > 3:\n",
    "        if order_of_magnitude % 3:\n",
    "            text += num2text(number//10**(order_of_magnitude -(order_of_magnitude % 3))) + ' ' + power_2_text[10**(3*(order_of_magnitude//3))]\n",
    "            text += num2text(number%10**(order_of_magnitude -(order_of_magnitude % 3)))\n",
    "        else:\n",
    "            text += num2text(number//(10**(order_of_magnitude-3))) + ' ' + power_2_text[10**(order_of_magnitude-3)]\n",
    "            text += num2text(number%(10**(order_of_magnitude-3)))\n",
    "    else:\n",
    "        if number//100:\n",
    "            text += ' ' + digit_2_text[number//100] + ' ' + power_2_text[100]\n",
    "        if number%100 < 20 and number%100 > 9:\n",
    "            text += ' ' + teen_2_text[number%100]\n",
    "            return text\n",
    "        if number%100 >= 20:\n",
    "            text += ' ' + tens_2_text[10*(number%100//10)]\n",
    "        if number%10:\n",
    "            text += ' ' + digit_2_text[number%10]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' nine hundred ninety nine quintillion one hundred thirteen quadrillion one hundred twenty three trillion one hundred eighty three billion four hundred twelve million three hundred fourty seven thousand two hundred thirty nine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2text(999113123183412347239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №2\n",
    "Преобразовать текст к нижнему или верхнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "data = [line.lower() for line in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №3\n",
    "Произвести токенезацию (токенами выступают слова), не учитывая знаки препинания (т.е. необходимо разбить текст на токены (лексемы)):\n",
    "* С помощью nltk.tokenize.RegexpTokenizer (тут необходимо вспомнить регулярные выражения [posix](https://ru.wikibooks.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D1%8B%D0%B5_%D0%B2%D1%8B%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "import nltk\n",
    "\n",
    "regtokenizer = nltk.tokenize.RegexpTokenizer(r'[A-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'i', 'well', 'prince', 'so', 'genoa', 'and', 'lucca', 'are', 'now']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for line in data:\n",
    "    tokens += regtokenizer.tokenize(line)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* С помощью nltk.word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'i', '``', 'well', ',', 'prince', ',', 'so', 'genoa', 'and']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "wordtokens = []\n",
    "for line in data:\n",
    "    wordtokens += nltk.tokenize.word_tokenize(line)\n",
    "wordtokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №4\n",
    "Произвести частотный анализ слов. Методы библиотек не использовать, т.е. алгоритм необходимо написать самостоятельно. Cохранить результат в выходной csv файл в порядке убывания частот. Формат выходного файла:\n",
    "\n",
    "| Word | Count |\n",
    "| --- | --- |\n",
    "| cat | 1000 |\n",
    "| dog | 300 |\n",
    "| .... | .... |\n",
    "| butterfly | 34 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "import pandas as pd\n",
    "frequency = dict()\n",
    "for token in tokens:\n",
    "    if token in frequency.keys():\n",
    "        frequency[token] += 1\n",
    "    else:\n",
    "        frequency[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame().from_dict(frequency, orient='index', columns=['Count'])\n",
    "export.to_csv('./word_frequency.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №5\n",
    "Произвести частотный анализ [N-грам](https://en.wikipedia.org/wiki/N-gram) и сохранить в выходной файл в порядке убывания частот (формат файла аналогичен предыдущему примеру, за исключением столбца Word, который заменяется на N-gram. В этой задаче можно использовать готовые библиотеки (смотрим состав библиотеки [NLTK](https://www.nltk.org/)). Формат выходного файла:\n",
    "\n",
    "| bigram | | Count |\n",
    "| --- | --- | --- |\n",
    "| cat | is | 1000 |\n",
    "| is | a | 300 |\n",
    "| .... | .... | .... |\n",
    "| the | fish | 34 |\n",
    "\n",
    "* Биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "bigram_frequency = dict()\n",
    "for i , token in enumerate(tokens):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if tokens[i-1] + ' ' + token in bigram_frequency.keys():\n",
    "        bigram_frequency[tokens[i-1] + ' ' + token] += 1\n",
    "    else:\n",
    "        bigram_frequency[tokens[i-1] + ' ' + token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame().from_dict(bigram_frequency, orient='index', columns=['Count'])\n",
    "export.to_csv('./bigram_frequency.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3-грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "trigram_frequency = dict()\n",
    "for i , token in enumerate(tokens):\n",
    "    if i == 0 or i == 1:\n",
    "        continue\n",
    "    if tokens[i-2] + ' ' + tokens[i-1] + ' ' + token in trigram_frequency.keys():\n",
    "        trigram_frequency[tokens[i-2] + ' ' + tokens[i-1] + ' ' + token] += 1\n",
    "    else:\n",
    "        trigram_frequency[tokens[i-2] + ' ' + tokens[i-1] + ' ' + token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame().from_dict(trigram_frequency, orient='index', columns=['Count'])\n",
    "export.to_csv('./trigram_frequency.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №6 (аналогично шагу 3)\n",
    "Произвести токенезацию, но уже учитывая знаки препинания:\n",
    "* С помощью nltk.tokenize.RegexpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'i', '\"', 'well', ',', 'prince', ',', 'so', 'genoa', 'and']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "regtokenizer1 = nltk.tokenize.RegexpTokenizer(r\"([A-Za-z0-9]+|[!\\\"#\\$%&\\\\'\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\\\\\\\\\]\\^_`\\{\\|\\}~\\.])\")\n",
    "tokens1 = []\n",
    "for line in data:\n",
    "    tokens1 += regtokenizer1.tokenize(line)\n",
    "tokens1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* С помощью nltk.word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'i', '``', 'well', ',', 'prince', ',', 'so', 'genoa', 'and']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "wordtokens1 = []\n",
    "for line in data:\n",
    "    wordtokens1 += nltk.tokenize.word_tokenize(line)\n",
    "wordtokens1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №7 (аналогично шагу 4)\n",
    "Произвести частотный анализ слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "frequency1 = dict()\n",
    "for token in tokens1:\n",
    "    if token in frequency1.keys():\n",
    "        frequency1[token] += 1\n",
    "    else:\n",
    "        frequency1[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame().from_dict(frequency1, orient='index', columns=['Count'])\n",
    "export.to_csv('./word_frequency_with_punct_tokenizer.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №8 (аналогично шагу 5)\n",
    "Произвести частотный анализ N-грам.\n",
    "\n",
    "* Биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "bigram_frequency1 = dict()\n",
    "for i , token in enumerate(tokens1):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if tokens1[i-1] + ' ' + token in bigram_frequency1.keys():\n",
    "        bigram_frequency1[tokens1[i-1] + ' ' + token] += 1\n",
    "    else:\n",
    "        bigram_frequency1[tokens1[i-1] + ' ' + token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame().from_dict(bigram_frequency1, orient='index', columns=['Count'])\n",
    "export.to_csv('./bigram_frequency_with_punct_tokenizer.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3-грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "trigram_frequency1 = dict()\n",
    "for i , token in enumerate(tokens1):\n",
    "    if i == 0 or i == 1:\n",
    "        continue\n",
    "    if tokens1[i-2] + ' ' + tokens1[i-1] + ' ' + token in trigram_frequency1.keys():\n",
    "        trigram_frequency1[tokens1[i-2] + ' ' + tokens1[i-1] + ' ' + token] += 1\n",
    "    else:\n",
    "        trigram_frequency1[tokens1[i-2] + ' ' + tokens1[i-1] + ' ' + token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.DataFrame().from_dict(trigram_frequency1, orient='index', columns=['Count'])\n",
    "export.to_csv('./trigram_frequency_with_punct_tokenizer.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №9 (используя результаты 5-го шага)\n",
    "Произвести генерацию текста на основе частот биграм. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "unused = {(key.split()[0],key.split()[1]) for key in bigram_frequency.keys()}\n",
    "text = ['i']\n",
    "for _ in range(40):\n",
    "    max_ = 0\n",
    "    max_t = ''  \n",
    "    for bigram in unused:\n",
    "        if text[-1] == bigram[0]:\n",
    "            if bigram_frequency[' '.join(bigram)] > max_:\n",
    "                max_ = bigram_frequency[' '.join(bigram)]\n",
    "                max_t = bigram[1]     \n",
    "    if max_t == '':\n",
    "        break   \n",
    "    unused.remove((text[-1],max_t))     \n",
    "    text.append(max_t)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am i have been in the same time to the french army and the emperor s face and that the old man who had been sent to be a man s a long time and he had not to him\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
